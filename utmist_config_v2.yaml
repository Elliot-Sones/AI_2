# UTMIST AI^2 Training Configuration v2 - Complete Config Edition
# ====================================================================
# All configuration in one place for reproducibility.
#
# PHASES:
#   "0a" - "0e" = Navigation training (movement first)
#   1 - 4 = Combat training (fighting skills)
#
# To switch phases: Change curriculum.phase below.
# To resume training: Set ppo_settings.model_checkpoint to the model filename.
# ====================================================================

settings:
  game_id: "warehouse_brawl"
  action_space: "discrete"

folders:
  parent_dir: "./results"
  model_name: "ppo_utmist_v2"

# ============================================================================
# CURRICULUM CONTROL - CHANGE THIS TO SWITCH PHASES
# ============================================================================
curriculum:
  phase: "0d"  # Full stage navigation (1-15 units, 3 targets)

# ============================================================================
# PHASE DEFINITIONS - All reward weights and opponent mixes
# ============================================================================
phases:
  # --------------------------------------------------------------------------
  # NAVIGATION PHASES (Phase 0) - Learn to move before fighting
  # --------------------------------------------------------------------------
  "0a":
    name: "Near Walking"
    description: "Walk to nearby targets from spawn (learn platform safety)"
    type: "navigation"
    navigation:
      min_dist: 1.0
      max_dist: 3.0          # Short distances - walking only
      fall_penalty: -3.0     # Penalty for falling off stage
      time_limit_seconds: 90 # Episode time limit
      multi_target: 1        # Number of targets per episode
    opponents:
      constant_agent: 1.0    # 100% frozen opponent

  "0b":
    name: "Platform Navigation"
    description: "Navigate main platform with medium distances"
    type: "navigation"
    navigation:
      min_dist: 2.0
      max_dist: 6.0
      fall_penalty: -15.0   # Balanced: discourages falls but allows calculated risks
      time_limit_seconds: 90
      multi_target: 1
    opponents:
      constant_agent: 1.0

  "0c":
    name: "Cross-Platform Jumps"
    description: "Navigate to distant targets (requires jumping)"
    type: "navigation"
    navigation:
      min_dist: 5.0
      max_dist: 10.0
      fall_penalty: -15.0  # Balanced penalty (same as 0b)
      time_limit_seconds: 90
      multi_target: 1
    opponents:
      constant_agent: 1.0

  "0d":
    name: "Full Stage Navigation"
    description: "Navigate to multiple targets anywhere on stage"
    type: "navigation"
    navigation:
      min_dist: 1.0
      max_dist: 15.0
      fall_penalty: -30.0  # Aggressive: near-zero falls goal
      time_limit_seconds: 60
      multi_target: 3
    opponents:
      constant_agent: 1.0

  "0e":
    name: "Speed Navigation"
    description: "Fast navigation under time pressure"
    type: "navigation"
    navigation:
      min_dist: 1.0
      max_dist: 15.0
      fall_penalty: -8.0
      time_limit_seconds: 30
      multi_target: 3
    opponents:
      constant_agent: 1.0

  # --------------------------------------------------------------------------
  # COMBAT PHASES (1-4) - Progressive skill building
  # --------------------------------------------------------------------------
  1:
    name: "Learn to Approach"
    description: "Focus on closing distance while staying on stage"
    type: "combat"
    rewards:
      distance: 2.0           # Strong reward for getting closer
      aggression: 0.5         # Small reward for attacking
      damage_dealt: 0.0       # No damage reward yet
      damage_taken: 0.0       # No damage penalty yet
      net_damage: 0.0
      win: 0.0
      knockout: -3.0          # Death penalty from day 1!
      edge_penalty: -1.0      # Strong edge avoidance
    opponents:
      random_agent: 0.9       # 90% easy opponents
      self_play: 0.1          # 10% self-play

  2:
    name: "Learn to Hit"
    description: "Focus on dealing damage and avoiding damage"
    type: "combat"
    rewards:
      distance: 0.5
      aggression: 0.2
      damage_dealt: 1.0       # Now rewarded for damage
      damage_taken: -0.5      # Penalize taking damage
      net_damage: 0.3
      win: 1.0
      knockout: 2.0
      edge_penalty: -1.0
    opponents:
      random_agent: 0.45
      self_play: 0.35
      edge_guard_agent: 0.05
      weapon_hunter_agent: 0.05
      recovery_punish_agent: 0.05
      noisy_based_agent: 0.05

  3:
    name: "Learn to Dominate"
    description: "Win through skill, not luck"
    type: "combat"
    rewards:
      distance: 0.1
      aggression: 0.5
      damage_dealt: 0.3
      damage_taken: -0.3
      net_damage: 0.3
      win: 8.0                # Strong win incentive
      knockout: 6.0           # KO bonus
      edge_penalty: -1.5      # Increased to reduce SDs
    opponents:
      random_agent: 0.20
      based_agent: 0.18
      constant_agent: 0.08
      clockwork_agent: 0.06
      self_play: 0.28
      edge_guard_agent: 0.06
      weapon_hunter_agent: 0.06
      recovery_punish_agent: 0.04
      noisy_based_agent: 0.04

  4:
    name: "Pure Competition"
    description: "Win-focused with diverse opponents (Operation Giant Slayer)"
    type: "combat"
    rewards:
      distance: 0.0
      aggression: 0.2
      damage_dealt: 0.2
      damage_taken: -0.1
      net_damage: 0.1
      win: 10.0               # Maximum win reward
      knockout: 5.0
      edge_penalty: -1.0
      clean_win_bonus: 5.0    # Extra bonus for positive net damage wins
    opponents:
      based_agent: 0.28
      random_agent: 0.08
      clockwork_agent: 0.08
      self_play: 0.32
      edge_guard_agent: 0.06
      weapon_hunter_agent: 0.06
      recovery_punish_agent: 0.06
      noisy_based_agent: 0.06

# ============================================================================
# Neural network architecture
# ============================================================================
policy_kwargs:
  net_arch: [512, 512, 256]

# Frame stacking for temporal context
frame_stack: 4

# ============================================================================
# PPO hyperparameters
# ============================================================================
ppo_settings:
  gamma: 0.99
  gae_lambda: 0.95
  
  # Learning rate schedule [initial, final]
  learning_rate: [3.0e-4, 1.0e-6]
  
  # Clip range schedule [initial, final]
  clip_range: [0.2, 0.05]
  
  # Entropy coefficient (encourages exploration)
  ent_coef: 0.01
  
  # For navigation phases, use lower entropy to reduce accidental falls
  nav_ent_coef: 0.05
  
  # Batch settings
  batch_size: 8192
  n_epochs: 8
  n_steps: 1024
  
  # Other settings
  vf_coef: 0.5
  max_grad_norm: 0.5
  
  # Training duration for this phase
  time_steps: 5000000
  
  # Set to "0" for new training, or filename to resume
  # Examples: "phase1_final", "phase4_final.zip", "nav_0a_final"
  model_checkpoint: "nav_0c_20251219_082313.zip"  # Best 0c model

# ============================================================================
# Environment settings
# ============================================================================
environment_settings:
  max_timesteps: 5400       # Max steps per episode (90 seconds at 60fps)
  render_mode: null
  resolution: "LOW"         # LOW, MEDIUM, HIGH
  n_envs: 48                # Parallel environments (Increased for 72-core CPU)

# ============================================================================
# Self-play settings
# ============================================================================
self_play_settings:
  save_freq: 500000         # Save model every N steps
  max_saved_models: 7       # Keep last N models for self-play pool

# ============================================================================
# Logging and evaluation
# ============================================================================
logging:
  log_freq: 10000           # Console log every N steps
  eval_freq: 500000         # Evaluation every N steps (Phase 3+ only)
  eval_episodes: 10         # Episodes per opponent during eval
  record_video_freq: 1000000 # Record demo video every N steps

# ============================================================================
# Opponent History (Simple Context)
# Turn temporal memory into spatial pattern recognition by appending
# a rolling buffer of opponent actions to the observation.
# ============================================================================
opponent_history:
  enabled: true             # Set to false to disable and use original obs
  history_length: 60        # ~2 seconds of opponent actions at 30fps
  action_dim: 10            # Size of UTMIST action space
