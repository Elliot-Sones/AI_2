# UTMIST AI^2 — Reinforcement Learning Fighting Game Agent

An autonomous fighting game agent built for the [UTMIST AI^2 tournament](https://utmist.gitlab.io/projects/ai-squared/), trained entirely through reinforcement learning. The agent learns to navigate a 2D platform stage, approach opponents, deal damage, and win matches — starting from zero game knowledge.

---

## Approach

Three design decisions define this agent: the choice of **PPO** as the learning algorithm, a custom **opponent modeling** system for reading opponent behavior, and **self-play** for robustness. These work together — PPO provides the stability needed for self-play's non-stationary training, and opponent modeling gives the policy the temporal context it needs to adapt mid-match.

### PPO

The UTMIST fighting game has extremely noisy reward signals — damage is bursty, deaths are sudden, and matches run 90 seconds. We chose **Proximal Policy Optimization** because its clipped objective prevents catastrophic policy updates that would erase learned behaviors. Algorithms like DQN or SAC are more sample-efficient in theory, but tend to collapse when reward variance is this high.

PPO is also naturally self-play compatible. As an on-policy method, it doesn't store old transitions in a replay buffer — so when the opponent distribution shifts (because the self-play pool updates), the algorithm adapts cleanly. This same property made PPO the backbone of OpenAI Five (Dota 2) and DeepMind's AlphaStar.

We use a **3-layer MLP** (`512 → 512 → 256`) rather than a CNN because the environment provides a structured feature vector (player positions, velocities, damage, lives), not raw pixels.

### Opponent Modeling

A standard MLP policy only sees the current frame. To recognize opponent patterns — does this opponent always attack after jumping? do they favor the left side? are they spamming the same move? — we implemented a **Simple Context** system that gives the agent a 2-second memory of opponent behavior:

```
Observation = [game_state (64 features)] + [opponent_history (60 × 10 = 600 features)]
```

A rolling buffer stores the **last 60 opponent actions** (~2 seconds at 30fps) as one-hot encoded vectors, flattened and appended directly to the observation. This converts temporal patterns into spatial features that the MLP can learn from — no recurrent networks needed.

**Why not LSTM?** Recurrent networks are powerful but significantly slower to train with PPO (backpropagation through time, sequence batching complexity). The simple context approach gives us the pattern recognition we need at a fraction of the training cost. The agent can "see" whether the opponent has been aggressive, passive, or predictable — and adjust its strategy accordingly.

During navigation phases, the history buffer is zeroed out (no opponent to track), but the observation space stays the same shape, enabling seamless weight transfer into combat.

### Self-Play

An agent that only trains against RandomAgent will develop strategies that exploit randomness but collapse against deliberate play. To prevent this, the agent trains against a **pool of its own past checkpoints** — saved every 500k steps, keeping the last 7.

Self-play percentage ramps up across phases as the agent matures:

| Phase | Self-Play % | Why |
|-------|-------------|-----|
| 1 | 10% | Focus on basic skills first |
| 2 | 40% | Force adaptation against improving self |
| 3 | 30% | Balance with diverse built-in opponents |
| 4 | 40% | Maximum competitive pressure |

---

## Training Pipeline

The core insight behind our training pipeline: **an agent that can't walk won't learn to fight**. Early experiments with direct combat training led to agents that immediately self-destructed by walking off the stage. The agent would spend millions of steps learning to *avoid dying* before it could even begin learning to *deal damage*.

Our solution is a 9-phase curriculum that builds skills incrementally, with reward shaping that evolves at each stage.

### Phase 0 — Navigation

Before any combat, the agent learns to move. We freeze the opponent at a random position on the stage and reward the agent for reaching it — turning the fighting game into a navigation task.

| Phase | Goal | Target Distance | Targets | Time Limit |
|-------|------|-----------------|---------|------------|
| **0a** | Walk to nearby positions | 1–2 units | 1 | 90s |
| **0b** | Jump to platforms | 3–5 units | 1 | 90s |
| **0c** | Navigate full stage | 5–12 units | 1 | 90s |
| **0d** | Reach 3 sequential targets | 1–12 units | 3 | 60s |
| **0e** | Speed runs under pressure | 1–12 units | 3 | 30s |

The fall penalty escalates across sub-phases (`-3.0` → `-8.0`), and entropy is set high (`0.1`) to encourage exploration of the full movement space. By Phase 0e, the agent can reliably traverse the entire stage — including platforms that require precise jump sequences — without dying.

The reachability map below was generated by recording every position the agent reached during navigation training. Green dots are grounded positions; scattered points show jump arcs. This confirmed full stage coverage before moving to combat.

<p align="center">
  <img src="images/aerial_reachability_cloud.png" width="80%" alt="Agent Reachability Map — 15,846 unique positions reached during navigation training"/>
</p>
<p align="center"><i>Full Agent Reachability Map — 15,846 unique positions visited across all navigation phases</i></p>

### Phase 1 — Learn to Approach

With navigation skills transferred, the agent enters combat. Phase 1 focuses purely on **closing distance** while staying alive. Damage rewards are disabled entirely — the agent is only rewarded for getting closer (`distance: 2.0`) and penalized for dying (`knockout: -3.0`).

**Opponents:** 90% RandomAgent, 10% self-play.

**Result:** The agent chases opponents across the stage without falling off. Self-destruct rate drops significantly compared to agents trained without Phase 0.

### Phase 2 — Learn to Hit

The reward function shifts to damage: `damage_dealt: 1.0`, `damage_taken: -0.5`. Distance reward drops to `0.5` — approach matters less now that the agent already knows how to close distance. Self-play increases to 40%.

**Result:** The agent develops basic attack patterns and learns to punish opponent openings. Win rate against ConstantAgent reaches **70%**.

### Phase 3 — Learn to Dominate

The reward pivots hard toward winning: `win: 8.0`, `knockout: 6.0`. The opponent pool diversifies to include all agent types (RandomAgent, BasedAgent, ConstantAgent, ClockworkAgent) plus 30% self-play. Periodic evaluation against all opponents begins at this phase.

**Result:** Consistent wins against RandomAgent and ClockworkAgent (**60%**). The agent begins competing with BasedAgent (the strongest built-in opponent).

### Phase 4 — Pure Competition

Maximum win reward (`10.0`) with a `clean_win_bonus: 5.0` for winning with positive net damage. The opponent mix is 40% BasedAgent + 40% self-play. Distance reward drops to zero — the agent must win through combat, not positioning.

**Result:** The agent demonstrates strategic behavior — retreating when at disadvantage, edge-guarding opponents, and adapting attack patterns based on opponent tendencies detected through the opponent history buffer.

The density heatmap below shows where the final agent spends its time during matches. The concentration near center stage and around platforms reveals learned stage control — staying safe while contesting advantageous positions.

<p align="center">
  <img src="images/aerial_heatmap.png" width="80%" alt="Reachability Density Heatmap — agent positional tendencies during combat"/>
</p>
<p align="center"><i>Reachability Density Heatmap — positional tendencies during combat. Warmer colors = higher visitation frequency.</i></p>

### Reward Shaping Across Phases

The reward function evolves across phases, gradually shifting what behavior the agent optimizes for:

| Reward Signal | Phase 1 | Phase 2 | Phase 3 | Phase 4 |
|--------------|---------|---------|---------|---------|
| Distance | **2.0** | 0.5 | 0.1 | 0.0 |
| Aggression | 0.5 | 0.2 | 0.5 | 0.2 |
| Damage Dealt | 0.0 | **1.0** | 0.3 | 0.2 |
| Damage Taken | 0.0 | -0.5 | -0.3 | -0.1 |
| Win Bonus | 0.0 | 1.0 | **8.0** | **10.0** |
| Knockout | -3.0 | 2.0 | **6.0** | 5.0 |
| Edge Penalty | -1.0 | -1.0 | -1.5 | -1.0 |
| Clean Win Bonus | — | — | — | **5.0** |

Key transitions:
- **Phase 1 → 2:** Damage rewards activate — approaching isn't enough, the agent must land hits
- **Phase 2 → 3:** Win reward jumps 8x — focus shifts from individual hits to match outcomes
- **Phase 3 → 4:** Distance reward drops to zero — the agent must win through combat, not positioning

---

## Results

| Opponent | Win Rate | What It Tests |
|----------|----------|---------------|
| ConstantAgent | **70%** | Approach and raw damage output against stationary target |
| RandomAgent | **60%** | Consistency against unpredictable, chaotic play |
| ClockworkAgent | **60%** | Pattern recognition and punishing fixed sequences |
| Phase 1 Model | **100%** | Skill progression — current agent dominates earlier versions |
| Phase 2 Model | **100%** | Skill progression — combat skills compound across phases |

---

## Technical Details

### Environment Wrapper Stack

The training environment is built from modular wrappers, each adding a specific capability:

```
SelfPlayWarehouseBrawl        Base fighting game environment
    │
    ├── Float32Wrapper         Normalizes observations to float32
    │
    ├── FrozenOpponentWrapper  [Phase 0] Freezes opponent as nav target, adds nav rewards
    │   ── OR ──
    ├── DamageTrackingWrapper  [Phase 1-4] Tracks damage dealt/received, SD vs KO deaths
    │
    ├── OpponentHistoryWrapper  Appends 60-frame opponent action buffer to observations
    │
    ├── Monitor                 Episode statistics (SB3 standard)
    │
    └── VecFrameStack           Stacks 4 consecutive frames for temporal context
```

The `DamageTrackingWrapper` distinguishes between **self-destructs** (deaths with <20 recent damage taken) and **knockouts** (deaths from opponent attacks). This distinction is critical for diagnosing training — a high SD rate means the agent needs more navigation training, while a high KO rate means it needs better defense.

---

## Getting Started

### Prerequisites

- Python 3.10+
- PyTorch (CUDA recommended)
- [UTMIST AI^2 environment](https://github.com/UTMIST/UTMIST-AI2-main) cloned into `UTMIST-AI2-main/`

### Installation

```bash
pip install stable-baselines3 torch gymnasium shimmy tensorboard tqdm scikit-video
```

For cloud GPU training (Vast.ai with RTX 4090):

```bash
bash setup_vast.sh
```

### Training

```bash
# 1. Set the phase in config.yaml (start with "0a")
# 2. Train
python train.py

# 3. Ctrl+C to stop — auto-saves model + records demo video
# 4. Update config.yaml: set next phase + model_checkpoint
# 5. Repeat: 0a → 0b → 0c → 0d → 0e → 1 → 2 → 3 → 4
```

### Evaluation

```bash
# Run 10 games against each opponent
python eval_10_games.py

# Watch agent play with video recording
python watch_games.py --opponent based --games 5

# Play against the agent yourself
python play_vs_agent.py
```

### Monitoring

```bash
tensorboard --logdir ./results/ppo_utmist/tb
```

---

## Project Structure

```
AI_2/
├── train.py              Main training script — PPO, curriculum, self-play
├── config.yaml           Centralized configuration (phases, rewards, hyperparameters)
├── setup_vast.sh         Cloud GPU setup script (Vast.ai)
├── eval_10_games.py      Automated evaluation against all opponents
├── watch_games.py        Watch and record agent matches
├── play_vs_agent.py      Human vs agent interactive play
├── TRAINING_GUIDE.md     Detailed training documentation
├── images/               Visualizations and analysis
├── videos/               Demo recordings
└── results/
    └── ppo_utmist/
        └── model/        Trained models (phase 1–4)
```

---

## Tech Stack

| Component | Technology |
|-----------|------------|
| RL Algorithm | PPO ([Stable Baselines 3](https://stable-baselines3.readthedocs.io/)) |
| Neural Network | PyTorch — 3-layer MLP (512 → 512 → 256) |
| Training Infra | 32 parallel environments, Vast.ai RTX 4090 |
| Monitoring | TensorBoard |
| Game Environment | UTMIST AI^2 (Gymnasium-compatible) |
